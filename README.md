# GPT From Scratch
Building a GPT model from scratch using only basic python libraries. No huggingface, no torch.nn, but I do use the autograd utility of pytorch. The rest, tokenizer, optimizer, dataloader, parallel training, cuda kernels, etc. , are all implemented from scratch. All the code are written by myself without the help of LLMs. In this world when everything can be completed by AI for you, we are becoming increasingly seperated from the underlying principles, why everything works. As a believer of first principles, I decide that the ability to build the wheel is very important but somewhat underencouraged in today's world. So I build this repo.
